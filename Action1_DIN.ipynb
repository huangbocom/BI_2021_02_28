{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       user_id  merchant_id label origin  prob  age_range  gender\n",
      "0        16497         1203   0.0  train   NaN          0       1\n",
      "1         1950          946   0.0  train   NaN          2       0\n",
      "2        10829         2278   0.0  train   NaN          3       0\n",
      "3         7974          951   0.0  train   NaN          0       1\n",
      "4        14604         1892   0.0  train   NaN          7       0\n",
      "...        ...          ...   ...    ...   ...        ...     ...\n",
      "23888     2157         1748   nan   test   0.0          0       0\n",
      "23889     2673          798   nan   test   0.0          3       0\n",
      "23890    11847          639   nan   test   0.0          2       1\n",
      "23891    11847         3953   nan   test   0.0          2       1\n",
      "23892    19079         2954   nan   test   0.0          4       0\n",
      "\n",
      "[23893 rows x 7 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-d1dbc7821b16>:115: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  temp = groups['user_id', 'item_id', 'cat_id', 'brand_id'].nunique().reset_index().rename(columns={'user_id':'m2', 'item_id':'m3', 'cat_id':'m4', 'brand_id':'m5'})\n",
      "<ipython-input-10-d1dbc7821b16>:129: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  temp = groups['item_id', 'cat_id', 'brand_id'].nunique().reset_index().rename(columns={'item_id':'um2', 'cat_id':'um3', 'brand_id':'um4'}) #统计item_id, cat_id, brand_id唯一个数\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        user_id  merchant_id       um9\n",
      "0             0          186  0.883333\n",
      "1             0          202  0.000000\n",
      "2             0          523  0.000000\n",
      "3             0          774  0.000000\n",
      "4             0          956  0.000000\n",
      "...         ...          ...       ...\n",
      "635221    19111         2874  0.000000\n",
      "635222    19111         3833  0.000000\n",
      "635223    19111         4480  0.000000\n",
      "635224    19111         4522  0.000000\n",
      "635225    19111         4950  0.000000\n",
      "\n",
      "[635226 rows x 3 columns]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-d1dbc7821b16>:160: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  temp=pd.DataFrame(user_log.groupby('user_id')['merchant_id','action_type'].agg(lambda x:list(x)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       user_id  merchant_id label origin  prob    u1   u2  u3  u4  u5  ...  \\\n",
      "0        16497         1203   0.0  train   0.0    46   29  12  16  16  ...   \n",
      "1         1950          946   0.0  train   0.0   365  198  46  46  45  ...   \n",
      "2        10829         2278   0.0  train   0.0    47   31  14  15  17  ...   \n",
      "3         7974          951   0.0  train   0.0   234  105  23  35  36  ...   \n",
      "4        14604         1892   0.0  train   0.0   186  106  34  40  39  ...   \n",
      "...        ...          ...   ...    ...   ...   ...  ...  ..  ..  ..  ...   \n",
      "23888     2157         1748   nan   test   0.0   128   97  28  39  40  ...   \n",
      "23889     2673          798   nan   test   0.0  1286  540  55  93  96  ...   \n",
      "23890    11847          639   nan   test   0.0     9    8   7   7   7  ...   \n",
      "23891    11847         3953   nan   test   0.0     9    8   7   7   7  ...   \n",
      "23892    19079         2954   nan   test   0.0   197   85  36  39  40  ...   \n",
      "\n",
      "       age_4  age_5  age_6  age_7  age_8  g_0  g_1  g_2  \\\n",
      "0          0      0      0      0      0    0    1    0   \n",
      "1          0      0      0      0      0    1    0    0   \n",
      "2          0      0      0      0      0    1    0    0   \n",
      "3          0      0      0      0      0    0    1    0   \n",
      "4          0      0      0      1      0    1    0    0   \n",
      "...      ...    ...    ...    ...    ...  ...  ...  ...   \n",
      "23888      0      0      0      0      0    1    0    0   \n",
      "23889      0      0      0      0      0    1    0    0   \n",
      "23890      0      0      0      0      0    0    1    0   \n",
      "23891      0      0      0      0      0    0    1    0   \n",
      "23892      1      0      0      0      0    1    0    0   \n",
      "\n",
      "                                        hist_merchant_id  \\\n",
      "0      [3735, 1203, 3490, 2968, 3510, 3510, 3388, 610...   \n",
      "1      [1867, 3891, 141, 420, 3891, 420, 141, 1867, 1...   \n",
      "2      [604, 1807, 2950, 604, 2101, 1807, 1807, 1807,...   \n",
      "3      [644, 644, 951, 644, 644, 644, 3176, 951, 644,...   \n",
      "4      [1885, 40, 172, 1727, 1727, 1892, 1727, 1727, ...   \n",
      "...                                                  ...   \n",
      "23888  [4770, 4770, 3490, 3490, 215, 215, 215, 215, 2...   \n",
      "23889  [4172, 2418, 3734, 3212, 2418, 4346, 4172, 417...   \n",
      "23890  [1955, 203, 4842, 4842, 3953, 639, 639, 4308, ...   \n",
      "23891  [1955, 203, 4842, 4842, 3953, 639, 639, 4308, ...   \n",
      "23892  [4375, 1318, 2580, 3169, 4375, 1892, 3223, 431...   \n",
      "\n",
      "                                        hist_action_type  \n",
      "0      [1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
      "1      [4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
      "2      [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
      "3      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, ...  \n",
      "4      [1, 1, 1, 4, 1, 1, 1, 1, 1, 4, 1, 4, 3, 1, 1, ...  \n",
      "...                                                  ...  \n",
      "23888  [1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
      "23889  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
      "23890                        [1, 1, 1, 1, 3, 3, 1, 1, 1]  \n",
      "23891                        [1, 1, 1, 1, 3, 3, 1, 1, 1]  \n",
      "23892  [1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 1, ...  \n",
      "\n",
      "[23893 rows x 50 columns]\n",
      "       user_id  merchant_id  prob   u1   u2  u3  u4  u5        u6     u7  ...  \\\n",
      "0        16497         1203   0.0   46   29  12  16  16  4.933333   45.0  ...   \n",
      "1         1950          946   0.0  365  198  46  46  45  5.866667  313.0  ...   \n",
      "2        10829         2278   0.0   47   31  14  15  17  5.833333   42.0  ...   \n",
      "3         7974          951   0.0  234  105  23  35  36  5.833333  177.0  ...   \n",
      "4        14604         1892   0.0  186  106  34  40  39  5.866667  147.0  ...   \n",
      "...        ...          ...   ...  ...  ...  ..  ..  ..       ...    ...  ...   \n",
      "17832    18547         3825   0.0   40   24  13  14  15  4.916667   27.0  ...   \n",
      "17833    18693         3443   0.0   15   12  10  10  10  6.000000   10.0  ...   \n",
      "17834     4034         4172   0.0   16   14   8  11  12  5.833333   15.0  ...   \n",
      "17835    16017          993   0.0   33   25  12  15  15  4.916667   31.0  ...   \n",
      "17836     7308         4723   0.0   94   50  18  24  22  5.766667   80.0  ...   \n",
      "\n",
      "       age_4  age_5  age_6  age_7  age_8  g_0  g_1  g_2  \\\n",
      "0          0      0      0      0      0    0    1    0   \n",
      "1          0      0      0      0      0    1    0    0   \n",
      "2          0      0      0      0      0    1    0    0   \n",
      "3          0      0      0      0      0    0    1    0   \n",
      "4          0      0      0      1      0    1    0    0   \n",
      "...      ...    ...    ...    ...    ...  ...  ...  ...   \n",
      "17832      0      0      1      0      0    0    1    0   \n",
      "17833      0      0      0      0      0    1    0    0   \n",
      "17834      0      0      0      0      0    1    0    0   \n",
      "17835      0      0      1      0      0    1    0    0   \n",
      "17836      0      0      0      0      0    1    0    0   \n",
      "\n",
      "                                        hist_merchant_id  \\\n",
      "0      [3735, 1203, 3490, 2968, 3510, 3510, 3388, 610...   \n",
      "1      [1867, 3891, 141, 420, 3891, 420, 141, 1867, 1...   \n",
      "2      [604, 1807, 2950, 604, 2101, 1807, 1807, 1807,...   \n",
      "3      [644, 644, 951, 644, 644, 644, 3176, 951, 644,...   \n",
      "4      [1885, 40, 172, 1727, 1727, 1892, 1727, 1727, ...   \n",
      "...                                                  ...   \n",
      "17832  [2647, 826, 4137, 2941, 4137, 4137, 4137, 4137...   \n",
      "17833  [937, 4289, 2812, 4289, 3119, 3119, 3119, 3119...   \n",
      "17834  [3685, 1737, 1737, 1588, 4172, 1737, 1588, 353...   \n",
      "17835  [584, 993, 993, 993, 1138, 1138, 4497, 1138, 1...   \n",
      "17836  [3662, 3662, 3662, 3662, 3662, 3662, 3662, 366...   \n",
      "\n",
      "                                        hist_action_type  \n",
      "0      [1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
      "1      [4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
      "2      [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
      "3      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, ...  \n",
      "4      [1, 1, 1, 4, 1, 1, 1, 1, 1, 4, 1, 4, 3, 1, 1, ...  \n",
      "...                                                  ...  \n",
      "17832  [1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, ...  \n",
      "17833  [1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 3, 3, 3, 3, 1, ...  \n",
      "17834  [1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
      "17835  [1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
      "17836  [3, 1, 3, 1, 1, 1, 1, 3, 1, 1, 1, 1, 3, 1, 3, ...  \n",
      "\n",
      "[17837 rows x 48 columns]\n",
      "user_id\n",
      "14488\n",
      "merchant_id\n",
      "1856\n",
      "prob\n",
      "1\n",
      "u1\n",
      "825\n",
      "u2\n",
      "539\n",
      "u3\n",
      "124\n",
      "u4\n",
      "246\n",
      "u5\n",
      "237\n",
      "u6\n",
      "184\n",
      "u7\n",
      "772\n",
      "u8\n",
      "17\n",
      "u9\n",
      "79\n",
      "u10\n",
      "161\n",
      "m1\n",
      "805\n",
      "m2\n",
      "406\n",
      "m3\n",
      "292\n",
      "m4\n",
      "56\n",
      "m5\n",
      "35\n",
      "m6\n",
      "757\n",
      "m7\n",
      "23\n",
      "m8\n",
      "208\n",
      "m9\n",
      "163\n",
      "m10\n",
      "1\n",
      "um1\n",
      "170\n",
      "um2\n",
      "94\n",
      "um3\n",
      "22\n",
      "um4\n",
      "12\n",
      "um5\n",
      "166\n",
      "um6\n",
      "8\n",
      "um7\n",
      "10\n",
      "um8\n",
      "26\n",
      "um9\n",
      "184\n",
      "r1\n",
      "3265\n",
      "r2\n",
      "1413\n",
      "r3\n",
      "394\n",
      "age_0\n",
      "2\n",
      "age_2\n",
      "2\n",
      "age_3\n",
      "2\n",
      "age_4\n",
      "2\n",
      "age_5\n",
      "2\n",
      "age_6\n",
      "2\n",
      "age_7\n",
      "2\n",
      "age_8\n",
      "2\n",
      "g_0\n",
      "2\n",
      "g_1\n",
      "2\n",
      "g_2\n",
      "2\n",
      "action_type\n",
      "1\n",
      "M= 500\n",
      "[SparseFeat(name='user_id', vocabulary_size=19112, embedding_dim=10, use_hash=False, dtype='int32', embeddings_initializer=<tensorflow.python.keras.initializers.initializers_v1.RandomNormal object at 0x0000018915DC3B50>, embedding_name='user_id', group_name='default_group', trainable=True), SparseFeat(name='merchant_id', vocabulary_size=4995, embedding_dim=8, use_hash=False, dtype='int32', embeddings_initializer=<tensorflow.python.keras.initializers.initializers_v1.RandomNormal object at 0x00000189705E89D0>, embedding_name='merchant_id', group_name='default_group', trainable=True), DenseFeat(name='prob', dimension=1, dtype='float32'), DenseFeat(name='u1', dimension=1, dtype='float32'), DenseFeat(name='u2', dimension=1, dtype='float32'), DenseFeat(name='u3', dimension=1, dtype='float32'), DenseFeat(name='u4', dimension=1, dtype='float32'), DenseFeat(name='u5', dimension=1, dtype='float32'), DenseFeat(name='u6', dimension=1, dtype='float32'), DenseFeat(name='u7', dimension=1, dtype='float32'), DenseFeat(name='u8', dimension=1, dtype='float32'), DenseFeat(name='u9', dimension=1, dtype='float32'), DenseFeat(name='u10', dimension=1, dtype='float32'), DenseFeat(name='m1', dimension=1, dtype='float32'), DenseFeat(name='m2', dimension=1, dtype='float32'), DenseFeat(name='m3', dimension=1, dtype='float32'), DenseFeat(name='m4', dimension=1, dtype='float32'), DenseFeat(name='m5', dimension=1, dtype='float32'), DenseFeat(name='m6', dimension=1, dtype='float32'), DenseFeat(name='m7', dimension=1, dtype='float32'), DenseFeat(name='m8', dimension=1, dtype='float32'), DenseFeat(name='m9', dimension=1, dtype='float32'), DenseFeat(name='m10', dimension=1, dtype='float32'), DenseFeat(name='um1', dimension=1, dtype='float32'), DenseFeat(name='um2', dimension=1, dtype='float32'), DenseFeat(name='um3', dimension=1, dtype='float32'), DenseFeat(name='um4', dimension=1, dtype='float32'), DenseFeat(name='um5', dimension=1, dtype='float32'), DenseFeat(name='um6', dimension=1, dtype='float32'), DenseFeat(name='um7', dimension=1, dtype='float32'), DenseFeat(name='um8', dimension=1, dtype='float32'), DenseFeat(name='um9', dimension=1, dtype='float32'), DenseFeat(name='r1', dimension=1, dtype='float32'), DenseFeat(name='r2', dimension=1, dtype='float32'), DenseFeat(name='r3', dimension=1, dtype='float32'), DenseFeat(name='age_0', dimension=1, dtype='float32'), DenseFeat(name='age_2', dimension=1, dtype='float32'), DenseFeat(name='age_3', dimension=1, dtype='float32'), DenseFeat(name='age_4', dimension=1, dtype='float32'), DenseFeat(name='age_5', dimension=1, dtype='float32'), DenseFeat(name='age_6', dimension=1, dtype='float32'), DenseFeat(name='age_7', dimension=1, dtype='float32'), DenseFeat(name='age_8', dimension=1, dtype='float32'), DenseFeat(name='g_0', dimension=1, dtype='float32'), DenseFeat(name='g_1', dimension=1, dtype='float32'), DenseFeat(name='g_2', dimension=1, dtype='float32'), SparseFeat(name='action_type', vocabulary_size=5, embedding_dim=4, use_hash=False, dtype='int32', embeddings_initializer=<tensorflow.python.keras.initializers.initializers_v1.RandomNormal object at 0x00000189088894C0>, embedding_name='action_type', group_name='default_group', trainable=True), VarLenSparseFeat(sparsefeat=SparseFeat(name='hist_merchant_id', vocabulary_size=19112, embedding_dim=8, use_hash=False, dtype='int32', embeddings_initializer=<tensorflow.python.keras.initializers.initializers_v1.RandomNormal object at 0x00000189061B7AF0>, embedding_name='merchant_id', group_name='default_group', trainable=True), maxlen=500, combiner='mean', length_name=None, weight_name=None, weight_norm=True), VarLenSparseFeat(sparsefeat=SparseFeat(name='hist_action_type', vocabulary_size=5, embedding_dim=4, use_hash=False, dtype='int32', embeddings_initializer=<tensorflow.python.keras.initializers.initializers_v1.RandomNormal object at 0x00000189267A97C0>, embedding_name='action_type', group_name='default_group', trainable=True), maxlen=500, combiner='mean', length_name=None, weight_name=None, weight_norm=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (lambda_2), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'attention_sequence_pooling_layer_2/local_activation_unit_2/kernel:0' shape=(40, 1) dtype=float32>\n",
      "  <tf.Variable 'attention_sequence_pooling_layer_2/local_activation_unit_2/bias:0' shape=(1,) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "100%|███████████████████████████████████████████████████████████████████████| 17837/17837 [00:00<00:00, 1619872.26it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 17837/17837 [00:00<00:00, 1192346.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Users\\huangbo\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:432: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "UnimplementedError",
     "evalue": " Cast string to float is not supported\n\t [[node binary_crossentropy/Cast (defined at <ipython-input-10-d1dbc7821b16>:230) ]] [Op:__inference_train_function_18764]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-d1dbc7821b16>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    228\u001b[0m         \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[0mtrain_model_input\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfea\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_model_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;31m# 转换test__model_input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Users\\huangbo\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Users\\huangbo\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Users\\huangbo\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Users\\huangbo\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    838\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 840\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    841\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Users\\huangbo\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Users\\huangbo\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32me:\\Users\\huangbo\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32me:\\Users\\huangbo\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Users\\huangbo\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnimplementedError\u001b[0m:  Cast string to float is not supported\n\t [[node binary_crossentropy/Cast (defined at <ipython-input-10-d1dbc7821b16>:230) ]] [Op:__inference_train_function_18764]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# 使用DIN模型\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import log_loss\n",
    "#ctr 0.8以上的版本用的模块\n",
    "from deepctr.feature_column import SparseFeat,VarLenSparseFeat,DenseFeat,get_feature_names\n",
    "from deepctr.models import DIN, DIEN, DSIN\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "\n",
    "#from deepctr.feature_column import SparseFeat,get_feature_names\n",
    "\n",
    "# 用户行为，使用format1进行加载\n",
    "# 加载全量样本\n",
    "\"\"\"\n",
    "user_log = pd.read_csv('./data_format1/user_log_format1.csv', dtype={'time_stamp':'str'})\n",
    "user_info = pd.read_csv('./data_format1/user_info_format1.csv')\n",
    "train_data1 = pd.read_csv('./data_format1/train_format1.csv')\n",
    "submission = pd.read_csv('./data_format1/test_format1.csv')\n",
    "\"\"\"\n",
    "# 加载小样本\n",
    "user_log = pd.read_csv('./sample_user_log.csv', dtype={'time_stamp':'str'})\n",
    "user_info = pd.read_csv('./sample_user_info.csv')\n",
    "train_data1 = pd.read_csv('./train.csv')\n",
    "submission = pd.read_csv('./test.csv')\n",
    "train_data = pd.read_csv('./train_format2.csv')\n",
    "\n",
    "train_data1['origin'] = 'train'\n",
    "submission['origin'] = 'test'\n",
    "matrix = pd.concat([train_data1, submission], ignore_index=True, sort=False)\n",
    "#print(matrix)\n",
    "\n",
    "# 改名merchant_id（原列名seller_id）\n",
    "user_log.rename(columns={'seller_id':'merchant_id'}, inplace=True)\n",
    "# 格式化\n",
    "user_log['user_id'] = user_log['user_id'].astype('int32')\n",
    "user_log['merchant_id'] = user_log['merchant_id'].astype('int32')\n",
    "user_log['item_id'] = user_log['item_id'].astype('int32')\n",
    "user_log['cat_id'] = user_log['cat_id'].astype('int32')\n",
    "user_log['brand_id'].fillna(0, inplace=True)\n",
    "user_log['brand_id'] = user_log['brand_id'].astype('int32')\n",
    "user_log['time_stamp'] = pd.to_datetime(user_log['time_stamp'], format='%H%M')\n",
    "\n",
    "# 对离散特征做LabelEncoder\n",
    "lbe_merchant_id=LabelEncoder()\n",
    "lbe_merchant_id.fit(np.r_[0,user_log['merchant_id'].values])\n",
    "user_log['merchant_id']=lbe_merchant_id.transform(user_log['merchant_id'])\n",
    "matrix['merchant_id']=lbe_merchant_id.transform(matrix['merchant_id'])\n",
    "\n",
    "lbe_user_id=LabelEncoder()\n",
    "user_log['user_id']=lbe_user_id.fit_transform(user_log['user_id'])\n",
    "user_info['user_id']=lbe_user_id.transform(user_info['user_id'])\n",
    "matrix['user_id']=lbe_user_id.transform(matrix['user_id'])\n",
    "\n",
    "lbe_item_id=LabelEncoder()\n",
    "user_log['item_id']=lbe_item_id.fit_transform(user_log['item_id'])\n",
    "lbe_cat_id=LabelEncoder()\n",
    "user_log['cat_id']=lbe_cat_id.fit_transform(user_log['cat_id'])\n",
    "lbe_brand_id=LabelEncoder()\n",
    "user_log['brand_id']=lbe_brand_id.fit_transform(user_log['brand_id'])\n",
    "\n",
    "user_log['merchant_id'].max(),user_log['user_id'].max()\n",
    "matrix = matrix.merge(user_info, on='user_id', how='left')\n",
    "\n",
    "# 1 for <18; 2 for [18,24]; 3 for [25,29]; 4 for [30,34]; 5 for [35,39]; 6 for [40,49]; 7 and 8 for >= 50; 0 and NULL for unknown\n",
    "matrix['age_range'].fillna(0, inplace=True)\n",
    "# 0:female, 1:male, 2:unknown\n",
    "matrix['gender'].fillna(2, inplace=True)\n",
    "matrix['age_range'] = matrix['age_range'].astype('int8')\n",
    "matrix['gender'] = matrix['gender'].astype('int8')\n",
    "matrix['label'] = matrix['label'].astype('str')\n",
    "matrix['user_id'] = matrix['user_id'].astype('int32')\n",
    "matrix['merchant_id'] = matrix['merchant_id'].astype('int32')\n",
    "del user_info, train_data1\n",
    "gc.collect()\n",
    "print(matrix)\n",
    "\n",
    "# User特征处理\n",
    "groups = user_log.groupby(['user_id'])\n",
    "# 用户交互行为数量 u1\n",
    "temp = groups.size().reset_index().rename(columns={0:'u1'})\n",
    "matrix = matrix.merge(temp, on='user_id', how='left')\n",
    "# 使用agg 基于列的聚合操作，统计唯一值的个数 item_id, cat_id, merchant_id, brand_id\n",
    "#temp = groups['item_id', 'cat_id', 'merchant_id', 'brand_id'].nunique().reset_index().rename(columns={'item_id':'u2', 'cat_id':'u3', 'merchant_id':'u4', 'brand_id':'u5'})\n",
    "temp = groups['item_id'].agg([('u2', 'nunique')]).reset_index()\n",
    "matrix = matrix.merge(temp, on='user_id', how='left')\n",
    "temp = groups['cat_id'].agg([('u3', 'nunique')]).reset_index()\n",
    "matrix = matrix.merge(temp, on='user_id', how='left')\n",
    "temp = groups['merchant_id'].agg([('u4', 'nunique')]).reset_index()\n",
    "matrix = matrix.merge(temp, on='user_id', how='left')\n",
    "temp = groups['brand_id'].agg([('u5', 'nunique')]).reset_index()\n",
    "matrix = matrix.merge(temp, on='user_id', how='left')\n",
    "\n",
    "# 时间间隔特征 u6 按照小时\n",
    "temp = groups['time_stamp'].agg([('F_time', 'min'), ('L_time', 'max')]).reset_index()\n",
    "temp['u6'] = (temp['L_time'] - temp['F_time']).dt.seconds/3600\n",
    "matrix = matrix.merge(temp[['user_id', 'u6']], on='user_id', how='left')\n",
    "# 统计action_type为0，1，2，3的个数（原始操作，没有补0）\n",
    "temp = groups['action_type'].value_counts().unstack().reset_index().rename(columns={0:'u7', 1:'u8', 2:'u9', 3:'u10'})\n",
    "matrix = matrix.merge(temp, on='user_id', how='left')\n",
    "#print(matrix)\n",
    "\n",
    "# 商家特征处理\n",
    "groups = user_log.groupby(['merchant_id'])\n",
    "# 商家被交互行为数量 m1\n",
    "temp = groups.size().reset_index().rename(columns={0:'m1'})\n",
    "matrix = matrix.merge(temp, on='merchant_id', how='left')\n",
    "# 统计商家被交互的user_id, item_id, cat_id, brand_id 唯一值\n",
    "temp = groups['user_id', 'item_id', 'cat_id', 'brand_id'].nunique().reset_index().rename(columns={'user_id':'m2', 'item_id':'m3', 'cat_id':'m4', 'brand_id':'m5'})\n",
    "matrix = matrix.merge(temp, on='merchant_id', how='left')\n",
    "# 统计商家被交互的action_type 唯一值\n",
    "temp = groups['action_type'].value_counts().unstack().reset_index().rename(columns={0:'m6', 1:'m7', 2:'m8', 3:'m9'})\n",
    "matrix = matrix.merge(temp, on='merchant_id', how='left')\n",
    "# 按照merchant_id 统计随机负采样的个数\n",
    "temp = train_data[train_data['label']==-1].groupby(['merchant_id']).size().reset_index().rename(columns={0:'m10'})\n",
    "matrix = matrix.merge(temp, on='merchant_id', how='left')\n",
    "#print(matrix)\n",
    "\n",
    "# 按照user_id, merchant_id分组\n",
    "groups = user_log.groupby(['user_id', 'merchant_id'])\n",
    "temp = groups.size().reset_index().rename(columns={0:'um1'}) #统计行为个数\n",
    "matrix = matrix.merge(temp, on=['user_id', 'merchant_id'], how='left')\n",
    "temp = groups['item_id', 'cat_id', 'brand_id'].nunique().reset_index().rename(columns={'item_id':'um2', 'cat_id':'um3', 'brand_id':'um4'}) #统计item_id, cat_id, brand_id唯一个数\n",
    "matrix = matrix.merge(temp, on=['user_id', 'merchant_id'], how='left')\n",
    "temp = groups['action_type'].value_counts().unstack().reset_index().rename(columns={0:'um5', 1:'um6', 2:'um7', 3:'um8'})#统计不同action_type唯一个数\n",
    "matrix = matrix.merge(temp, on=['user_id', 'merchant_id'], how='left')\n",
    "temp = groups['time_stamp'].agg([('first', 'min'), ('last', 'max')]).reset_index()\n",
    "temp['um9'] = (temp['last'] - temp['first']).dt.seconds/3600\n",
    "temp.drop(['first', 'last'], axis=1, inplace=True)\n",
    "print(temp)\n",
    "print('-'*100)\n",
    "matrix = matrix.merge(temp, on=['user_id', 'merchant_id'], how='left') #统计时间间隔\n",
    "#print(matrix)\n",
    "\n",
    "#用户购买点击比\n",
    "matrix['r1'] = matrix['u9']/matrix['u7'] \n",
    "#商家购买点击比\n",
    "matrix['r2'] = matrix['m8']/matrix['m6'] \n",
    "#不同用户不同商家购买点击比\n",
    "matrix['r3'] = matrix['um7']/matrix['um5']\n",
    "matrix.fillna(0, inplace=True)\n",
    "# # 修改age_range字段名称为 age_0, age_1, age_2... age_8\n",
    "temp = pd.get_dummies(matrix['age_range'], prefix='age')\n",
    "matrix = pd.concat([matrix, temp], axis=1)\n",
    "temp = pd.get_dummies(matrix['gender'], prefix='g')\n",
    "matrix = pd.concat([matrix, temp], axis=1)\n",
    "matrix.drop(['age_range', 'gender'], axis=1, inplace=True)\n",
    "#print(matrix)\n",
    "\n",
    "lbe_action_type={0:1,1:2,2:3,3:4}\n",
    "user_log['action_type']=user_log['action_type'].map(lbe_action_type)\n",
    "# 用户行为sequence\n",
    "# 把user_log里同user的这些数据合并成一个list\n",
    "temp=pd.DataFrame(user_log.groupby('user_id')['merchant_id','action_type'].agg(lambda x:list(x)))\n",
    "# 列名称改成hist_merchant_id 和 hist_action_type \n",
    "temp.columns=['hist_merchant_id','hist_action_type']\n",
    "#print(temp)\n",
    "matrix = matrix.merge(temp, on=['user_id'], how='left') #统计时间间隔\n",
    "print(matrix)\n",
    "\n",
    "# 截取，不缺到定长M个\n",
    "M=500\n",
    "for feature in ['hist_merchant_id','hist_action_type']:\n",
    "    matrix[feature]=matrix[feature].map(lambda x:np.array(x+[0]*(M-len(x)))[:M])\n",
    "\n",
    "# 分割训练数据和测试数据\n",
    "train_data = matrix[matrix['origin'] == 'train'].drop(['origin'], axis=1)\n",
    "test_data = matrix[matrix['origin'] == 'test'].drop(['label', 'origin'], axis=1)\n",
    "train_X, train_y = train_data.drop(['label'], axis=1), train_data['label']\n",
    "print(train_X)\n",
    "\n",
    "\n",
    "train_X['action_type']=3\n",
    "feature_columns = []\n",
    "for column in train_X.columns:\n",
    "  if column != 'hist_merchant_id' and column != 'hist_action_type':\n",
    "    print(column)\n",
    "    num = train_X[column].nunique()\n",
    "    if num > 10000:\n",
    "        dim = 10\n",
    "    else:\n",
    "        if num > 1000:\n",
    "            dim = 8\n",
    "        else:\n",
    "            dim = 4\n",
    "    print(num)\n",
    "    if column  == 'user_id':\n",
    "        feature_columns += [SparseFeat(column, 19111+1, embedding_dim=dim)]\n",
    "    elif column  == 'merchant_id':\n",
    "        feature_columns += [SparseFeat(column, 4994+1, embedding_dim=dim)]\n",
    "    elif column  == 'action_type':\n",
    "        feature_columns += [SparseFeat(column, 4+1, embedding_dim=dim)]\n",
    "    else:\n",
    "        feature_columns += [DenseFeat(column, 1)]\n",
    "\n",
    "#print(train_X['hist_merchant_id'].shape)\n",
    "#M = len(train_X['hist_merchant_id'])\n",
    "\n",
    "print('M=', M)\n",
    "\n",
    "# maxlen为历史信息的长度，vocabulary_size为onehot的长度\n",
    "#feature_columns += [VarLenSparseFeat('hist_merchant_id', maxlen=M, vocabulary_size=19112, embedding_dim=8, embedding_name='merchant_id'),\n",
    "#                   VarLenSparseFeat('hist_action_type', maxlen=M, vocabulary_size=5, embedding_dim=4, embedding_name='action_type')]\n",
    "feature_columns += [VarLenSparseFeat (sparsefeat = SparseFeat('hist_merchant_id', vocabulary_size=19112, embedding_dim=8, embedding_name='merchant_id'),maxlen=M ),\n",
    "                   VarLenSparseFeat (sparsefeat = SparseFeat('hist_action_type',  vocabulary_size=5, embedding_dim=4, embedding_name='action_type'),maxlen=M )]\n",
    "hist_features=['merchant_id','action_type']\n",
    "print(feature_columns)\n",
    "\n",
    "# 使用DIN模型\n",
    "model=DIN(feature_columns, hist_features)\n",
    "# 使用Adam优化器，二分类的交叉熵\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['binary_crossentropy'])\n",
    "\n",
    "# 组装train_model_input，得到feature names，将train_X转换为字典格式\n",
    "feature_names=list(train_X.columns)\n",
    "train_model_input = {name:train_X[name].values for name in feature_names}\n",
    "# histroy输入必须是二维数组\n",
    "from tqdm import tqdm\n",
    "for fea in ['hist_merchant_id','hist_action_type']:\n",
    "    l = []\n",
    "    for i in tqdm(train_model_input[fea]):\n",
    "        l.append(i)\n",
    "    train_model_input[fea]=np.array(l)\n",
    "history = model.fit(train_model_input, train_y, verbose=1, epochs=10, validation_split=0.2)\n",
    "\n",
    "# 转换test__model_input\n",
    "test_data['action_type']=3\n",
    "test_model_input = {name:test_data[name].values for name in feature_names}\n",
    "from tqdm import tqdm\n",
    "for fea in ['hist_merchant_id','hist_action_type']:\n",
    "    l = []\n",
    "    for i in tqdm(test_model_input[fea]):\n",
    "        l.append(i)\n",
    "    test_model_input[fea]=np.array(l)\n",
    "\n",
    "# 得到预测结果\n",
    "prob = model.predict(test_model_input)\n",
    "submission['prob'] = prob\n",
    "submission.drop(['origin'], axis=1, inplace=True)\n",
    "submission.to_csv('prediction.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
